<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kings-ai-rg.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kings-ai-rg.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-28T14:08:11+00:00</updated><id>https://kings-ai-rg.github.io/feed.xml</id><title type="html">Kings AI Reading Group</title><subtitle>Kings AI Reading Group </subtitle><entry><title type="html">Agentic AI systems - Promises, Risks and the Paths Forward</title><link href="https://kings-ai-rg.github.io/blog/2025/agentic-AI/" rel="alternate" type="text/html" title="Agentic AI systems - Promises, Risks and the Paths Forward"/><published>2025-07-04T12:00:00+00:00</published><updated>2025-07-04T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/agentic-AI</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/agentic-AI/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/nicole-obretincheva-b78619209/">Nicole Obretincheva</a></strong> presented a review on Agentic AI systems, which includes an overview of work presented by Microsoft, OWASP, IBM, and the EU AI Act.</p> <p><strong><em>Abstract</em></strong></p> <p>Agentic AI systems are becoming increasingly widely used, and with that comes a new set of risks. In this talk, Nicole provides a brief overview of what these agentic AI systems are, the types of threats posed by agentic AI (drawing on work from OWASP and Microsoft), and how risk mitigation strategies are starting to take shape to address issues posed by agentic AI. The talk also explores how existing governance frameworks in the EU and the UK apply to these systems.</p>]]></content><author><name></name></author><category term="talks"/><category term="paper-review"/><summary type="html"><![CDATA[Presented by Nicole Obretincheva]]></summary></entry><entry><title type="html">A Survey of Cognitive Distortion Detection and Classification in NLP</title><link href="https://kings-ai-rg.github.io/blog/2025/cognitive-distortion-detection-copy/" rel="alternate" type="text/html" title="A Survey of Cognitive Distortion Detection and Classification in NLP"/><published>2025-06-20T12:00:00+00:00</published><updated>2025-06-20T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/cognitive-distortion-detection%20copy</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/cognitive-distortion-detection-copy/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/archie-sage-3388bb260/">Archie Sage</a></strong> presented their ongoing work surrounding Cognitive Distortion Detection and Classification in NLP, which is currently under review.</p> <p><strong><em>Abstract</em></strong></p> <p>As interest in the application of natural language processing (NLP) techniques to mental health grows, a growing body of work explores the automatic detection and classification of cognitive distortions (CDs). CDs are negatively biased or inaccurate thought patterns that adversely affect the way an individual perceives themselves and the world around them. Identifying and addressing them is an important part of therapy. Despite its momentum, the field remains fragmented, with inconsistencies in CD taxonomies, task formulations, and evaluation practices. This survey reviews 38 studies spanning two decades, providing a structured overview of modelling approaches, datasets, and evaluation strategies. We propose a canonical CD taxonomy, summarise standard task setups, and highlight open challenges to support more coherent and reproducible research in this emerging area.</p>]]></content><author><name></name></author><category term="talks"/><category term="under-review"/><summary type="html"><![CDATA[Presented by Archie Sage]]></summary></entry><entry><title type="html">Reproducibility The New Frontier in AI Governance</title><link href="https://kings-ai-rg.github.io/blog/2025/reproducibility-the-new-frontier-in-ai-governance-copy/" rel="alternate" type="text/html" title="Reproducibility The New Frontier in AI Governance"/><published>2025-06-13T12:00:00+00:00</published><updated>2025-06-13T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/reproducibility-the-new-frontier-in-ai-governance%20copy</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/reproducibility-the-new-frontier-in-ai-governance-copy/"><![CDATA[<p><strong><a href="https://openreview.net/profile?id=~Israel_Mason-Williams1">Israel Mason-Williams</a></strong> presented their ongoing work surrounding AI governance which was accepted at the ICML Workshop on Technical AI Governance.</p> <p><strong><em>Abstract</em></strong></p> <p>Policymakers for AI are responsible for delivering effective governance mechanisms that can provide oversight into safety concerns. However, the information environment offered to policymakers is characterized by an unnecessarily low signal-to-noise ratio, favouring regulatory capture and creating deep uncertainty and divides on which risks should be prioritized from a governance perspective. We posit that the current speed of publication in AI combined with the lack of strong scientific standards, via weak reproducibility protocols, effectively erodes the power of policymakers to enact meaningful policy and governance protocols. Our paper outlines how AI research could adopt stricter reproducibility guidelines to assist governance endeavours and improve consensus on the risk landscapes posed by AI. We evaluate the forthcoming reproducibility crisis within AI research through the lens of reproducibility crises in other scientific domains and provide a commentary on how adopting preregistration, increased statistical power and negative result publication reproducibility protocols can enable effective AI governance. While we maintain that AI governance must be reactive due to AI’s significant societal implications we argue that policymakers and governments must consider reproducibility protocols as a core tool in the governance arsenal and demand higher standards for AI research.</p>]]></content><author><name></name></author><category term="talks"/><category term="ongoing-work"/><summary type="html"><![CDATA[Presented by Israel Mason-Williams]]></summary></entry><entry><title type="html">The Biology of a Large Language Model (Anthropic)</title><link href="https://kings-ai-rg.github.io/blog/2025/biology-llms-anthropic/" rel="alternate" type="text/html" title="The Biology of a Large Language Model (Anthropic)"/><published>2025-04-11T12:00:00+00:00</published><updated>2025-04-11T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/biology-llms-anthropic</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/biology-llms-anthropic/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/nathaliekirch/">Nathalie Kirch</a></strong> presented an overview of the recent Anthropic release: On the Biology of a Large Language Model <a class="citation" href="#lindsey2025biology">(Lindsey et al., 2025)</a>.</p> <p><strong><em>Abstract</em></strong></p> <p>In this talk, we explored the recent Anthropic Thread: On the Biology of a Large Language Model. It was a thought-provoking investigation into the internal structures, behaviors, and emergent properties of large language models (LLMs). The metaphor between biological systems and artificial intelligence was particularly strong and, this presentation explored how LLMs—like biological organisms—develop internal representations, organize “thought,” and respond adaptively to stimuli via a mechanistic perspective.</p>]]></content><author><name></name></author><category term="talks"/><category term="paper-presentation"/><summary type="html"><![CDATA[Presented by Nathalie Kirch]]></summary></entry><entry><title type="html">Reflections on The International AI Safety Report</title><link href="https://kings-ai-rg.github.io/blog/2025/international-AI-safety-report/" rel="alternate" type="text/html" title="Reflections on The International AI Safety Report"/><published>2025-03-07T12:00:00+00:00</published><updated>2025-03-07T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/international-AI-safety-report</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/international-AI-safety-report/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/israelfmw/">Israel Mason-Williams</a></strong>, <strong><a href="https://www.linkedin.com/in/nathaliekirch/">Nathalie Kirch</a></strong> and <strong><a href="https://www.linkedin.com/in/archie-sage-3388bb260/">Archie Sage</a></strong> presented a summary of the International AI Safety Report <a class="citation" href="#bengio2025international">(Bengio et al., 2025)</a></p> <p><strong><em>Abstract</em></strong></p> <p>This series of talks presented key insights from the latest International Safety Report, a comprehensive analysis of the three sections comprising Capabilities of general-purpose AI, Risks, and Technical approaches to risk management. Drawing on data from government agencies, industry leaders, and international organizations, the report highlights critical opinions surrounding Safe AI development. Antendees gained a perspective on evolving safety standards, regulatory frameworks, and the role of technology and collaboration in mitigating risk.</p> <p><strong>Updates</strong></p> <p>The International Safety Report is an iterative document, the reflections of this talk may not represent future versions of this document.</p>]]></content><author><name></name></author><category term="talks"/><category term="meta-research"/><category term="governance"/><category term="policy"/><summary type="html"><![CDATA[Presented by Israel Mason-Williams,Nathalie Kirch and Archie Sage.]]></summary></entry><entry><title type="html">DeepSeek V1, V2 and V3 Deep Dive</title><link href="https://kings-ai-rg.github.io/blog/2025/DeepSeek/" rel="alternate" type="text/html" title="DeepSeek V1, V2 and V3 Deep Dive"/><published>2025-02-14T12:00:00+00:00</published><updated>2025-02-14T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/DeepSeek</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/DeepSeek/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/avyav-kumar-singh/">Avyav Singh</a></strong> presented the collection of papers that led to the innovative appraoch adopted to create DeepSeek V1 and V2 <a class="citation" href="#guo2025deepseek">(Guo et al., 2025)</a> <a class="citation" href="#liu2024deepseek">(Liu et al., 2024)</a>, <a class="citation" href="#liu2024deepseek_v3">(Liu et al., 2024)</a>, <a class="citation" href="#gloeckle2024better">(Gloeckle et al., 2024)</a> and <a class="citation" href="#shao2024deepseekmath">(Shao et al., 2024)</a></p> <p><strong><em>Abstract</em></strong></p> <p>This talk explored the rapid evolution of DeepSeek’s foundational models from V1 through V3, highlighting key architectural advancements and capabilities. We explored in depth the key innovations which enabled the sucess of DeepSeek namely the introduced Mixture-of-Experts (MoE) architecture for scalable compute and dynamic routing, Multi-head Latent Attention and auxiliary-loss-free strategy for load balancing.</p>]]></content><author><name></name></author><category term="talks"/><category term="paper-presentation"/><summary type="html"><![CDATA[Presented by Avyav Singh]]></summary></entry><entry><title type="html">NeurIPS 2024 Recap</title><link href="https://kings-ai-rg.github.io/blog/2025/neurips-recap/" rel="alternate" type="text/html" title="NeurIPS 2024 Recap"/><published>2025-01-17T12:00:00+00:00</published><updated>2025-01-17T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/neurips-recap</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/neurips-recap/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/israelfmw/">Israel Mason-Williams</a></strong> and <strong><a href="https://www.linkedin.com/in/nathaliekirch/">Nathalie Kirch</a></strong> present on thier time at NeurIPS 2024.</p> <p><strong><em>Abstract</em></strong></p> <p>This meeting discussed talks and papers at NeurIPS 2024 that Israel and Nathalie saw that are of interest to the group. The papers covered were: ‘Generalization: Shortcuts, Spuriousness, and Stability’, ‘Universal Convergence’, ‘Initialization in GNNs’ and ‘Ablation Schemes for Interpretability’.</p>]]></content><author><name></name></author><category term="talks"/><category term="conference-update"/><summary type="html"><![CDATA[Presented by Israel Mason-Williams and Nathalie Kirch]]></summary></entry><entry><title type="html">Generative Flow Networks (GFlowNets)</title><link href="https://kings-ai-rg.github.io/blog/2024/Generative-Flow-Networks/" rel="alternate" type="text/html" title="Generative Flow Networks (GFlowNets)"/><published>2024-11-15T12:00:00+00:00</published><updated>2024-11-15T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2024/Generative-Flow-Networks</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2024/Generative-Flow-Networks/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/avyav-kumar-singh/">Avyav Singh</a></strong> presented the collection of papers that elucidate Generative Flow Networks <a class="citation" href="#hu2023amortizing">(Hu et al., 2023)</a> <a class="citation" href="#bengio2023gflownet">(Bengio et al., 2023)</a>, <a class="citation" href="#madan2023learning">(Madan et al., 2023)</a> and <a class="citation" href="#malkin2022trajectory">(Malkin et al., 2022)</a>.</p> <p><strong><em>Abstract</em></strong></p> <p>This talk explored the development of Generative Flow Networks (GFlowNets). We dissucssed how the use of amortized Bayesian inference to sample intractable posteriors, achieved through LLMs fine-tuning and diversity prioritised reinforcement learning algorithms enables data-efficient adaptation of LLMs.</p>]]></content><author><name></name></author><category term="talks"/><category term="paper-presentation"/><summary type="html"><![CDATA[Presented by Avyav Singh]]></summary></entry><entry><title type="html">Neural Network Compression - The Functional Perspective (+ Extensions)</title><link href="https://kings-ai-rg.github.io/blog/2024/functional-perspective/" rel="alternate" type="text/html" title="Neural Network Compression - The Functional Perspective (+ Extensions)"/><published>2024-11-08T12:00:00+00:00</published><updated>2024-11-08T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2024/functional-perspective</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2024/functional-perspective/"><![CDATA[<p><strong><a href="https://openreview.net/profile?id=~Israel_Mason-Williams1">Israel Mason-Williams</a></strong> presented and discussed the paper Neural Network Compression: The Functional Perspective (+ Extensions) which was accepted at PML4LRS Workshop at ICLR 2024 <a class="citation" href="#mason-williams2024neural">(Mason-Williams, 2024)</a> and with extensions accepted at NeurIPS Sci4DL Workshop <a class="citation" href="#mason-williams2024knowledge">(Mason-Williams et al., 2024)</a>.</p> <p><strong><em>Abstract</em></strong></p> <p>Compression techniques, such as Knowledge distillation, Pruning, and Quantization reduce the computational costs of model inference and enable on-edge machine learning. The efficacy of compression methods is often evaluated through the proxy of accuracy and loss to understand similarity of the compressed model. This study aims to explore the functional divergence between compressed and uncompressed models. The results indicate that Quantization and Pruning create models that are functionally similar to the original model. In contrast, Knowledge distillation creates models that do not functionally approximate their teacher models. The compressed model resembles the dissimilarity of function observed in independently trained models. Therefore, it is verified, via a functional understanding, that Knowledge distillation is not a compression method. Thus, leading to the definition of Knowledge distillation as a training regulariser given that no knowledge is distilled from a teacher to a student.</p>]]></content><author><name></name></author><category term="talks"/><category term="published-paper"/><summary type="html"><![CDATA[Presented by Israel Mason-Williams]]></summary></entry></feed>