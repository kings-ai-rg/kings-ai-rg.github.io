<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kings-ai-rg.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kings-ai-rg.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-04T22:14:20+00:00</updated><id>https://kings-ai-rg.github.io/feed.xml</id><title type="html">Kings AI Reading Group</title><subtitle>Kings AI Reading Group </subtitle><entry><title type="html">On the Biology of a Large Language Model (Anthropic)</title><link href="https://kings-ai-rg.github.io/blog/2025/biology-llms-anthropic/" rel="alternate" type="text/html" title="On the Biology of a Large Language Model (Anthropic)"/><published>2025-04-11T12:00:00+00:00</published><updated>2025-04-11T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/biology-llms-anthropic</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/biology-llms-anthropic/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/nathaliekirch/">Nathalie Kirch</a></strong> will be presenting an overview of the recent Anthropic release: On the Biology of a Large Language Model<a class="citation" href="#lindsey2025biology">(Lindsey et al., 2025)</a>.</p> <p><strong><em>Abstract</em></strong></p> <p>In this talk, we explore Anthropic Thread: On the Biology of a Large Language Model, a thought-provoking investigation into the internal structures, behaviors, and emergent properties of large language models (LLMs). Drawing a metaphor between biological systems and artificial intelligence, this presentation explores how LLMs—like biological organisms—develop internal representations, organize “thought,” and respond adaptively to stimuli via a mechanistic perspective.</p>]]></content><author><name></name></author><category term="talks"/><category term="paper-presentation"/><summary type="html"><![CDATA[Presented by Nathalie Kirch]]></summary></entry><entry><title type="html">Reflections on International AI Safety Report</title><link href="https://kings-ai-rg.github.io/blog/2025/international-AI-safety-report/" rel="alternate" type="text/html" title="Reflections on International AI Safety Report"/><published>2025-03-07T12:00:00+00:00</published><updated>2025-03-07T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/international-AI-safety-report</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/international-AI-safety-report/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/israelfmw/">Israel Mason-Williams</a></strong>, <strong><a href="https://www.linkedin.com/in/nathaliekirch/">Nathalie Kirch</a></strong> and <strong><a href="https://www.linkedin.com/in/archie-sage-3388bb260/">Archie Sage</a></strong> will be presenting a summary of the of International AI Safety Report <a class="citation" href="#bengio2025international">(Bengio et al., 2025)</a></p> <p><strong><em>Abstract</em></strong></p> <p>This series of talks presents key insights from the latest International Safety Report, a comprehensive analysis of the three sections comprising Capabilities of general-purpose AI, Risks, and Technical approaches to risk management. Drawing on data from government agencies, industry leaders, and international organizations, the report highlights critical opinions surrounding Safe AI development. Attendees will gain a perspective on evolving safety standards, regulatory frameworks, and the role of technology and cross-border collaboration in mitigating risk.</p> <p><strong>Updates</strong></p> <p>The International Safety Report is an iterative document, the reflections of this talk may not represent future versions of this document.</p>]]></content><author><name></name></author><category term="talks"/><category term="meta-research"/><category term="governance"/><category term="policy"/><summary type="html"><![CDATA[Presented by Israel Mason-Williams,Nathalie Kirch and Archie Sage.]]></summary></entry><entry><title type="html">NeurIPS 2024 Recap</title><link href="https://kings-ai-rg.github.io/blog/2025/neurips-recap/" rel="alternate" type="text/html" title="NeurIPS 2024 Recap"/><published>2025-01-17T12:00:00+00:00</published><updated>2025-01-17T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2025/neurips-recap</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2025/neurips-recap/"><![CDATA[<p><strong><a href="https://www.linkedin.com/in/israelfmw/">Israel Mason-Williams</a></strong> and <strong><a href="https://www.linkedin.com/in/nathaliekirch/">Nathalie Kirch</a></strong> will be presenting about their time at NeurIPS 2024.</p> <p><strong><em>Abstract</em></strong></p> <p>This meeting discussed talks and papers at NeurIPS 2024 that Israel and Nathalie saw that are of interest to the group. The papers covered were: ‘Generalization: Shortcuts, Spuriousness, and Stability’, ‘Universal Convergence’, ‘Initialization in GNNs’ and ‘Ablation Scehmes for Interpretability’.</p>]]></content><author><name></name></author><category term="talks"/><category term="conference-update"/><summary type="html"><![CDATA[Presented by Israel Mason-Williams and Nathalie Kirch]]></summary></entry><entry><title type="html">Neural Network Compression - The Functional Perspective (+ Extensions)</title><link href="https://kings-ai-rg.github.io/blog/2024/functional-perspective/" rel="alternate" type="text/html" title="Neural Network Compression - The Functional Perspective (+ Extensions)"/><published>2024-11-08T12:00:00+00:00</published><updated>2024-11-08T12:00:00+00:00</updated><id>https://kings-ai-rg.github.io/blog/2024/functional-perspective</id><content type="html" xml:base="https://kings-ai-rg.github.io/blog/2024/functional-perspective/"><![CDATA[<p><strong><a href="https://openreview.net/profile?id=~Israel_Mason-Williams1">Israel Mason-Williams</a></strong> will be presenting and discussing the paper Neural Network Compression: The Functional Perspective (+ Extensions) <a class="citation" href="#mason-williams2024neural">(Mason-Williams, 2024)</a> <a class="citation" href="#mason-williams2024knowledge">(Mason-Williams et al., 2024)</a>.</p> <p><strong><em>Abstract</em></strong></p> <p>Compression techniques, such as Knowledge distillation, Pruning, and Quantization reduce the computational costs of model inference and enable on-edge machine learning. The efficacy of compression methods is often evaluated through the proxy of accuracy and loss to understand similarity of the compressed model. This study aims to explore the functional divergence between compressed and uncompressed models. The results indicate that Quantization and Pruning create models that are functionally similar to the original model. In contrast, Knowledge distillation creates models that do not functionally approximate their teacher models. The compressed model resembles the dissimilarity of function observed in independently trained models. Therefore, it is verified, via a functional understanding, that Knowledge distillation is not a compression method. Thus, leading to the definition of Knowledge distillation as a training regulariser given that no knowledge is distilled from a teacher to a student.</p>]]></content><author><name></name></author><category term="talks"/><category term="published-paper"/><summary type="html"><![CDATA[Presented by Israel Mason-Williams]]></summary></entry></feed>